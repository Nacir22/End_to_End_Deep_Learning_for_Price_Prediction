{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyMmBeojWMETmehBE0P4I0E0"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"P-NoG9IsgyKA","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1713449234376,"user_tz":-120,"elapsed":4,"user":{"displayName":"nacir boutra","userId":"02229893040988056772"}},"outputId":"af2ef3bd-dd0a-443e-8e09-d6b7fc013f1d"},"outputs":[{"output_type":"stream","name":"stdout","text":["[Errno 2] No such file or directory: '/content/drive/MyDrive/End_to_End_Deep_Learning_for_Price_Prediction/notebooks'\n","/content\n"]}],"source":["%cd /content/drive/MyDrive/End_to_End_Deep_Learning_for_Price_Prediction/notebooks"]},{"cell_type":"code","source":["import os\n","import yaml\n","import pandas as pandas"],"metadata":{"id":"BayiiXlGh8-o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["current_path = os.getcwd()\n","                                   #1\n","path_to_yaml = os.path.join(current_path,   'data_exploration_config.yml')                     #2\n","\n","try:\n","    with open (path_to_yaml, 'r') as c_file:\n","        config = yaml.safe_load(c_file)                           #3\n","except Exception as e:\n","    print('Error reading the config file')\n","\n","load_from_scratch = config['general']['load_from_scratch']        #4\n","save_raw_dataframe = config['general']['save_raw_dataframe']\n","save_transformed_dataframe = config['general']['save_transformed_dataframe']\n","remove_bad_values = config['general']['remove_bad_values']\n","\n","\n","input_csv = config['file_names']['input_csv']\n","pickled_input_dataframe = config['file_names']['pickled_input_dataframe']\n","pickled_output_dataframe = config['file_names']['pickled_output_dataframe']\n","\n","\n","categorical_columns = config['columns']['categorical']\n","\n","continuous_columns = config['columns']['continuous']\n","data_columns = config['columns']['date']\n","text_columns = config['columns']['text']\n","excluded_columns = config['columns']['excluded']\n","\n","max_long = config['bounding_box']['max_long']\n","max_lat = config['bounding_box']['max_lat']\n","min_long = config['bounding_box']['min_long']\n","min_lat = config['bounding_box']['min_lat']\n","\n","newark_max_long = config['newark_bounding_box']['max_long']\n","newark_max_long = config['newark_bounding_box']['max_lat']\n","newark_max_long = config['newark_bounding_box']['min_long']\n","newark_max_long = config['newark_bounding_box']['min_lat']\n","\n","geo_columns =  config['geo_columns']\n"],"metadata":{"id":"g1K5-4WbiZ2L"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["rawpath = os.getcwd()\n","path = os.path.abspath(os.path.join(rawpath, '..', 'pickle'))"],"metadata":{"id":"UPNfLg4aiByK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df_loaded = pd.read_pickle(\"/mnt/data/my_dataframe.pkl\")"],"metadata":{"id":"XuC1v63qh3hx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Workflow\n","\n","    In a fresh Python Jupyter notebook, ingest the DataFrame that you saved as a pickle file in the previous milestone into a DataFrame.\n","    Like you did for your notebook for Milestone 1, use the sample exploration config file to define a YAML config file for your notebook.\n","    Add code to your notebook to ingest the config file you defined in the previous step and create Python variables for the parameters you defined in the config file.\n","    For each column category (categorical, continuous, and text), determine an appropriate replacement value to replace missing values.\n","        Think about the pros and cons of a fixed value versus a calculated value.\n","        Consider the conditions under which a value might be missing. Are there some columns where you think it would be better to have default values specific to the column instead of a category default?\n","    Write a function or functions to substitute the appropriate replacement value for each missing value.\n","    In addition to missing values, your dataset could contain invalid values. These are values that don’t make sense for a particular column. You will need to think about what constitutes valid values for each column, for example:\n","        For some of the continuous columns, the only valid values are nonnegative values.\n","        Some of the categorical columns have a finite set of valid values.\n","        Some columns do not have any obvious way to distinguish valid values. Think about which columns don’t readily lend themselves to checking for valid values and what you can do about values in these columns.\n","    Once you have identified the columns (or categories of columns) for which you can identify valid values, consider what you should do with invalid values in these columns.\n","        Should you replace an invalid value with a placeholder valid value, like you did for missing values, or with an identifier that will allow you to easily identify all the records that have at least one invalid column value?\n","        If you replace the invalid values with an identifier, then what should you do with the records that have this identifier?\n","    Write functions that identify and deal with invalid values for the columns where they can be identified.\n","        Remember that the output DataFrame that results from applying these functions should be ready to train a model once categorical and text values have been encoded as numeric values. Note that encoding the values in categorical and text columns will be done in Milestone 1 of the second liveProject, Train a DL Model.\n","        This means that once this milestone is done, every continuous column should contain only numeric values, and every categorical column should contain only valid tokens for that category.\n","    Think about how you will test these functions, particularly for the columns where the dataset is clean.\n","        You want your code to be robust and work with future versions of the dataset that may have anomalies that aren’t in the current version.\n","        What can you do to exercise the functions that deal with invalid values for columns that don’t have any invalid values in the current version of the dataset?\n","    Run your notebook and save the resulting DataFrame in a new pickle file that can be used as input to the next milestone.\n","\n"],"metadata":{"id":"3B7ML2nzjtgS"}},{"cell_type":"code","source":[],"metadata":{"id":"RPawafbdi33B"},"execution_count":null,"outputs":[]}]}